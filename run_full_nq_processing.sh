#!/bin/bash

# Process all 294 Natural Questions files using HuggingFace datasets approach
# This avoids numpy array serialization issues completely

echo "ðŸ”§ Processing all 294 Natural Questions files..."

# Activate conda environment
source ~/miniforge3/etc/profile.d/conda.sh
conda activate kg-dense

# Set M1 optimization environment variables
export KMP_DUPLICATE_LIB_OK=TRUE
export TOKENIZERS_PARALLELISM=false
export PYTORCH_ENABLE_MPS_FALLBACK=1
export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0

# Create logs directory
mkdir -p logs/processing

echo "ðŸ“Š Processing all Natural Questions files with HuggingFace datasets..."
python -c "
import os
import json
import logging
from pathlib import Path
from datasets import Dataset
import pyarrow.parquet as pq
import gc
import psutil
from datetime import datetime

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/Users/shivam/Documents/Shubham/Research project/logs/processing/natural_questions_processing.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def process_all_natural_questions():
    '''Process all 294 Natural Questions files using HuggingFace datasets'''
    
    downloads_dir = Path('/Users/shivam/Documents/Shubham/Research project/data/raw/qa/natural_questions/downloads')
    output_dir = Path('/Users/shivam/Documents/Shubham/Research project/data/processed/qa')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Find all parquet files
    parquet_files = []
    for file_path in downloads_dir.iterdir():
        if file_path.is_file() and not file_path.suffix and not file_path.name.endswith('.json') and not file_path.name.endswith('.lock'):
            try:
                pq.read_metadata(file_path)
                parquet_files.append(str(file_path))
            except:
                continue
    
    logger.info(f'Found {len(parquet_files)} parquet files')
    
    # Process all files
    all_examples = []
    batch_num = 1
    batch_size = 1000
    files_processed = 0
    start_time = datetime.now()
    
    for i, file_path in enumerate(parquet_files):
        logger.info(f'Processing file {i+1}/{len(parquet_files)}: {Path(file_path).name}')
        
        try:
            # Load with HuggingFace datasets
            dataset = Dataset.from_parquet(file_path)
            
            for example in dataset:
                # Extract safely with proper error handling
                try:
                    question_text = ''
                    if example.get('question') and isinstance(example['question'], dict):
                        question_text = str(example['question'].get('text', ''))
                    
                    document_title = ''
                    document_url = ''
                    if example.get('document') and isinstance(example['document'], dict):
                        document_title = str(example['document'].get('title', ''))
                        document_url = str(example['document'].get('url', ''))
                    
                    # Only process if we have essential fields
                    if example.get('id') and question_text:
                        processed_example = {
                            'id': str(example['id']),
                            'question': question_text,
                            'document_title': document_title,
                            'document_url': document_url,
                            'document_tokens': [],  # Skip complex tokens to avoid issues
                            'annotations': {}  # Skip complex annotations to avoid issues
                        }
                        
                        all_examples.append(processed_example)
                        
                        # Save batch when reaching batch size
                        if len(all_examples) >= batch_size:
                            output_file = output_dir / f'natural_questions_batch_{batch_num:04d}.json'
                            with open(output_file, 'w', encoding='utf-8') as f:
                                for ex in all_examples:
                                    f.write(json.dumps(ex, ensure_ascii=False) + '\n')
                            
                            logger.info(f'Saved batch {batch_num} with {len(all_examples)} examples')
                            all_examples = []
                            batch_num += 1
                            
                            # Memory management
                            gc.collect()
                
                except Exception as e:
                    # Skip problematic examples
                    continue
            
            files_processed += 1
            
            # Memory check
            memory_usage = psutil.virtual_memory().percent
            if memory_usage > 80:
                logger.warning(f'High memory usage: {memory_usage}%, forcing garbage collection')
                gc.collect()
            
        except Exception as e:
            logger.error(f'Error processing {file_path}: {e}')
    
    # Save remaining examples
    if all_examples:
        output_file = output_dir / f'natural_questions_batch_{batch_num:04d}.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            for ex in all_examples:
                f.write(json.dumps(ex, ensure_ascii=False) + '\n')
        logger.info(f'Saved final batch {batch_num} with {len(all_examples)} examples')
    
    # Calculate total examples
    total_examples = 0
    batch_files = list(output_dir.glob('natural_questions_batch_*.json'))
    for batch_file in batch_files:
        with open(batch_file, 'r') as f:
            total_examples += sum(1 for line in f if line.strip())
    
    # Save summary
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    summary = {
        'files_processed': files_processed,
        'examples_processed': total_examples,
        'batches_saved': batch_num,
        'duration_seconds': duration,
        'output_directory': str(output_dir)
    }
    
    summary_file = output_dir / 'natural_questions_summary.json'
    with open(summary_file, 'w') as f:
        json.dump(summary, f, indent=2)
    
    logger.info(f'Successfully processed {total_examples} examples from {files_processed} files')
    print(f'âœ… Natural Questions processing complete: {total_examples} examples from {files_processed} files')
    print(f'Duration: {duration:.2f} seconds')

if __name__ == '__main__':
    process_all_natural_questions()
"

echo "âœ… Natural Questions processing completed!"
echo "Check data/processed/qa/ for output files"
